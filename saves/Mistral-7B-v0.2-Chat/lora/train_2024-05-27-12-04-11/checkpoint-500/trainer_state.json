{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.8095238095238093,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0380952380952381,
      "grad_norm": 6.944857120513916,
      "learning_rate": 4.9992811366328926e-05,
      "loss": 4.7161,
      "step": 5
    },
    {
      "epoch": 0.0761904761904762,
      "grad_norm": 7.338119029998779,
      "learning_rate": 4.997124959943201e-05,
      "loss": 4.1929,
      "step": 10
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 7.696468353271484,
      "learning_rate": 4.993532709928075e-05,
      "loss": 3.4368,
      "step": 15
    },
    {
      "epoch": 0.1523809523809524,
      "grad_norm": 4.544597148895264,
      "learning_rate": 4.9885064524570665e-05,
      "loss": 2.567,
      "step": 20
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.1456966400146484,
      "learning_rate": 4.982049078084071e-05,
      "loss": 2.0952,
      "step": 25
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 2.2230734825134277,
      "learning_rate": 4.974164300384998e-05,
      "loss": 1.825,
      "step": 30
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.9935892820358276,
      "learning_rate": 4.964856653822122e-05,
      "loss": 1.6839,
      "step": 35
    },
    {
      "epoch": 0.3047619047619048,
      "grad_norm": 1.4216827154159546,
      "learning_rate": 4.954131491136362e-05,
      "loss": 1.6191,
      "step": 40
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 1.180444359779358,
      "learning_rate": 4.9419949802689666e-05,
      "loss": 1.4999,
      "step": 45
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 1.2254300117492676,
      "learning_rate": 4.92845410081439e-05,
      "loss": 1.488,
      "step": 50
    },
    {
      "epoch": 0.41904761904761906,
      "grad_norm": 1.1648893356323242,
      "learning_rate": 4.913516640006392e-05,
      "loss": 1.449,
      "step": 55
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 1.1051982641220093,
      "learning_rate": 4.897191188239667e-05,
      "loss": 1.4369,
      "step": 60
    },
    {
      "epoch": 0.49523809523809526,
      "grad_norm": 1.0394572019577026,
      "learning_rate": 4.8794871341296e-05,
      "loss": 1.3994,
      "step": 65
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.0529628992080688,
      "learning_rate": 4.8604146591129485e-05,
      "loss": 1.4229,
      "step": 70
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.9676260352134705,
      "learning_rate": 4.8399847315926e-05,
      "loss": 1.368,
      "step": 75
    },
    {
      "epoch": 0.6095238095238096,
      "grad_norm": 0.99428790807724,
      "learning_rate": 4.818209100629745e-05,
      "loss": 1.4061,
      "step": 80
    },
    {
      "epoch": 0.6476190476190476,
      "grad_norm": 1.008302927017212,
      "learning_rate": 4.795100289187099e-05,
      "loss": 1.3476,
      "step": 85
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 1.16199791431427,
      "learning_rate": 4.7706715869270635e-05,
      "loss": 1.4022,
      "step": 90
    },
    {
      "epoch": 0.7238095238095238,
      "grad_norm": 1.1098365783691406,
      "learning_rate": 4.74493704256897e-05,
      "loss": 1.4272,
      "step": 95
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.1421867609024048,
      "learning_rate": 4.717911455809782e-05,
      "loss": 1.3773,
      "step": 100
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1274964809417725,
      "learning_rate": 4.6896103688129385e-05,
      "loss": 1.3565,
      "step": 105
    },
    {
      "epoch": 0.8380952380952381,
      "grad_norm": 1.228756070137024,
      "learning_rate": 4.660050057270191e-05,
      "loss": 1.4167,
      "step": 110
    },
    {
      "epoch": 0.8761904761904762,
      "grad_norm": 1.3438767194747925,
      "learning_rate": 4.6292475210416106e-05,
      "loss": 1.3369,
      "step": 115
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 1.4329936504364014,
      "learning_rate": 4.597220474379125e-05,
      "loss": 1.389,
      "step": 120
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 1.1872899532318115,
      "learning_rate": 4.563987335739216e-05,
      "loss": 1.3591,
      "step": 125
    },
    {
      "epoch": 0.9904761904761905,
      "grad_norm": 1.1809924840927124,
      "learning_rate": 4.5295672171906364e-05,
      "loss": 1.3123,
      "step": 130
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 1.2801074981689453,
      "learning_rate": 4.49397991342324e-05,
      "loss": 1.3789,
      "step": 135
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.1991734504699707,
      "learning_rate": 4.4572458903642354e-05,
      "loss": 1.42,
      "step": 140
    },
    {
      "epoch": 1.1047619047619048,
      "grad_norm": 1.2432416677474976,
      "learning_rate": 4.419386273408428e-05,
      "loss": 1.3355,
      "step": 145
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 1.2829079627990723,
      "learning_rate": 4.3804228352691935e-05,
      "loss": 1.3584,
      "step": 150
    },
    {
      "epoch": 1.180952380952381,
      "grad_norm": 1.2974241971969604,
      "learning_rate": 4.3403779834572004e-05,
      "loss": 1.3237,
      "step": 155
    },
    {
      "epoch": 1.2190476190476192,
      "grad_norm": 1.3027797937393188,
      "learning_rate": 4.2992747473940556e-05,
      "loss": 1.2919,
      "step": 160
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 1.2726106643676758,
      "learning_rate": 4.2571367651683e-05,
      "loss": 1.3017,
      "step": 165
    },
    {
      "epoch": 1.2952380952380953,
      "grad_norm": 1.3997727632522583,
      "learning_rate": 4.213988269941362e-05,
      "loss": 1.3116,
      "step": 170
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.3294004201889038,
      "learning_rate": 4.169854076011292e-05,
      "loss": 1.2453,
      "step": 175
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 1.3473143577575684,
      "learning_rate": 4.124759564542295e-05,
      "loss": 1.3084,
      "step": 180
    },
    {
      "epoch": 1.4095238095238094,
      "grad_norm": 1.458443284034729,
      "learning_rate": 4.078730668968253e-05,
      "loss": 1.3098,
      "step": 185
    },
    {
      "epoch": 1.4476190476190476,
      "grad_norm": 1.3824541568756104,
      "learning_rate": 4.031793860078649e-05,
      "loss": 1.2579,
      "step": 190
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 1.440774917602539,
      "learning_rate": 3.9839761307954675e-05,
      "loss": 1.3433,
      "step": 195
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 1.5485877990722656,
      "learning_rate": 3.935304980649813e-05,
      "loss": 1.2508,
      "step": 200
    },
    {
      "epoch": 1.561904761904762,
      "grad_norm": 1.5197304487228394,
      "learning_rate": 3.8858083999671855e-05,
      "loss": 1.2134,
      "step": 205
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5307598114013672,
      "learning_rate": 3.835514853770505e-05,
      "loss": 1.2832,
      "step": 210
    },
    {
      "epoch": 1.638095238095238,
      "grad_norm": 1.5560933351516724,
      "learning_rate": 3.784453265410141e-05,
      "loss": 1.3367,
      "step": 215
    },
    {
      "epoch": 1.6761904761904762,
      "grad_norm": 1.5821003913879395,
      "learning_rate": 3.732652999930364e-05,
      "loss": 1.2754,
      "step": 220
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.5839283466339111,
      "learning_rate": 3.680143847181783e-05,
      "loss": 1.2333,
      "step": 225
    },
    {
      "epoch": 1.7523809523809524,
      "grad_norm": 1.4573651552200317,
      "learning_rate": 3.6269560046894766e-05,
      "loss": 1.2774,
      "step": 230
    },
    {
      "epoch": 1.7904761904761903,
      "grad_norm": 1.586928367614746,
      "learning_rate": 3.573120060286679e-05,
      "loss": 1.2047,
      "step": 235
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 1.5990291833877563,
      "learning_rate": 3.5186669745240026e-05,
      "loss": 1.2822,
      "step": 240
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.5958616733551025,
      "learning_rate": 3.463628062864312e-05,
      "loss": 1.2697,
      "step": 245
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 1.5590704679489136,
      "learning_rate": 3.4080349776734925e-05,
      "loss": 1.2566,
      "step": 250
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 1.6620049476623535,
      "learning_rate": 3.351919690017473e-05,
      "loss": 1.2494,
      "step": 255
    },
    {
      "epoch": 1.980952380952381,
      "grad_norm": 1.612534999847412,
      "learning_rate": 3.2953144712759545e-05,
      "loss": 1.2742,
      "step": 260
    },
    {
      "epoch": 2.019047619047619,
      "grad_norm": 1.7506952285766602,
      "learning_rate": 3.238251874583452e-05,
      "loss": 1.1955,
      "step": 265
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 1.6620182991027832,
      "learning_rate": 3.1807647161082795e-05,
      "loss": 1.2359,
      "step": 270
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 1.7702277898788452,
      "learning_rate": 3.122886056180284e-05,
      "loss": 1.2571,
      "step": 275
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.7801717519760132,
      "learning_rate": 3.064649180278152e-05,
      "loss": 1.2468,
      "step": 280
    },
    {
      "epoch": 2.1714285714285713,
      "grad_norm": 1.8492077589035034,
      "learning_rate": 3.006087579887244e-05,
      "loss": 1.2642,
      "step": 285
    },
    {
      "epoch": 2.2095238095238097,
      "grad_norm": 1.9339889287948608,
      "learning_rate": 2.9472349332389525e-05,
      "loss": 1.2248,
      "step": 290
    },
    {
      "epoch": 2.2476190476190476,
      "grad_norm": 1.829075813293457,
      "learning_rate": 2.8881250859426646e-05,
      "loss": 1.1951,
      "step": 295
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 1.8561389446258545,
      "learning_rate": 2.8287920315214643e-05,
      "loss": 1.2722,
      "step": 300
    },
    {
      "epoch": 2.323809523809524,
      "grad_norm": 1.778012990951538,
      "learning_rate": 2.7692698918627778e-05,
      "loss": 1.1981,
      "step": 305
    },
    {
      "epoch": 2.361904761904762,
      "grad_norm": 2.181413412094116,
      "learning_rate": 2.7095928975951913e-05,
      "loss": 1.1646,
      "step": 310
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.8973747491836548,
      "learning_rate": 2.649795368402735e-05,
      "loss": 1.1881,
      "step": 315
    },
    {
      "epoch": 2.4380952380952383,
      "grad_norm": 1.890434980392456,
      "learning_rate": 2.5899116932879534e-05,
      "loss": 1.2155,
      "step": 320
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 2.014005422592163,
      "learning_rate": 2.529976310795108e-05,
      "loss": 1.2307,
      "step": 325
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 2.0988333225250244,
      "learning_rate": 2.470023689204893e-05,
      "loss": 1.1606,
      "step": 330
    },
    {
      "epoch": 2.552380952380952,
      "grad_norm": 2.100167751312256,
      "learning_rate": 2.4100883067120475e-05,
      "loss": 1.2489,
      "step": 335
    },
    {
      "epoch": 2.5904761904761906,
      "grad_norm": 2.1795761585235596,
      "learning_rate": 2.3502046315972656e-05,
      "loss": 1.2028,
      "step": 340
    },
    {
      "epoch": 2.6285714285714286,
      "grad_norm": 1.7745872735977173,
      "learning_rate": 2.290407102404809e-05,
      "loss": 1.2568,
      "step": 345
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.1752665042877197,
      "learning_rate": 2.2307301081372224e-05,
      "loss": 1.1971,
      "step": 350
    },
    {
      "epoch": 2.704761904761905,
      "grad_norm": 1.9714869260787964,
      "learning_rate": 2.1712079684785363e-05,
      "loss": 1.241,
      "step": 355
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 1.9575260877609253,
      "learning_rate": 2.111874914057336e-05,
      "loss": 1.2004,
      "step": 360
    },
    {
      "epoch": 2.780952380952381,
      "grad_norm": 1.9858297109603882,
      "learning_rate": 2.0527650667610478e-05,
      "loss": 1.1715,
      "step": 365
    },
    {
      "epoch": 2.819047619047619,
      "grad_norm": 1.9855343103408813,
      "learning_rate": 1.993912420112756e-05,
      "loss": 1.2705,
      "step": 370
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 1.9681190252304077,
      "learning_rate": 1.935350819721849e-05,
      "loss": 1.1579,
      "step": 375
    },
    {
      "epoch": 2.895238095238095,
      "grad_norm": 2.1354870796203613,
      "learning_rate": 1.8771139438197168e-05,
      "loss": 1.2216,
      "step": 380
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 2.1335699558258057,
      "learning_rate": 1.819235283891721e-05,
      "loss": 1.2041,
      "step": 385
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 2.052349328994751,
      "learning_rate": 1.7617481254165487e-05,
      "loss": 1.197,
      "step": 390
    },
    {
      "epoch": 3.0095238095238095,
      "grad_norm": 2.1058759689331055,
      "learning_rate": 1.704685528724046e-05,
      "loss": 1.203,
      "step": 395
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 2.1825172901153564,
      "learning_rate": 1.648080309982528e-05,
      "loss": 1.2277,
      "step": 400
    },
    {
      "epoch": 3.085714285714286,
      "grad_norm": 2.0628573894500732,
      "learning_rate": 1.591965022326507e-05,
      "loss": 1.1842,
      "step": 405
    },
    {
      "epoch": 3.123809523809524,
      "grad_norm": 2.103325605392456,
      "learning_rate": 1.536371937135688e-05,
      "loss": 1.1202,
      "step": 410
    },
    {
      "epoch": 3.1619047619047618,
      "grad_norm": 2.229091167449951,
      "learning_rate": 1.4813330254759985e-05,
      "loss": 1.257,
      "step": 415
    },
    {
      "epoch": 3.2,
      "grad_norm": 2.236520767211914,
      "learning_rate": 1.426879939713322e-05,
      "loss": 1.2167,
      "step": 420
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 2.101222276687622,
      "learning_rate": 1.3730439953105243e-05,
      "loss": 1.1407,
      "step": 425
    },
    {
      "epoch": 3.276190476190476,
      "grad_norm": 2.2434046268463135,
      "learning_rate": 1.3198561528182183e-05,
      "loss": 1.2349,
      "step": 430
    },
    {
      "epoch": 3.314285714285714,
      "grad_norm": 2.1526167392730713,
      "learning_rate": 1.2673470000696364e-05,
      "loss": 1.156,
      "step": 435
    },
    {
      "epoch": 3.3523809523809525,
      "grad_norm": 2.1558902263641357,
      "learning_rate": 1.2155467345898602e-05,
      "loss": 1.1848,
      "step": 440
    },
    {
      "epoch": 3.3904761904761904,
      "grad_norm": 2.182997941970825,
      "learning_rate": 1.1644851462294957e-05,
      "loss": 1.1828,
      "step": 445
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 2.3308522701263428,
      "learning_rate": 1.114191600032815e-05,
      "loss": 1.1415,
      "step": 450
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 2.1698360443115234,
      "learning_rate": 1.064695019350187e-05,
      "loss": 1.2161,
      "step": 455
    },
    {
      "epoch": 3.5047619047619047,
      "grad_norm": 2.1594269275665283,
      "learning_rate": 1.0160238692045332e-05,
      "loss": 1.1855,
      "step": 460
    },
    {
      "epoch": 3.5428571428571427,
      "grad_norm": 2.2401270866394043,
      "learning_rate": 9.682061399213525e-06,
      "loss": 1.177,
      "step": 465
    },
    {
      "epoch": 3.580952380952381,
      "grad_norm": 2.2447125911712646,
      "learning_rate": 9.21269331031748e-06,
      "loss": 1.1482,
      "step": 470
    },
    {
      "epoch": 3.619047619047619,
      "grad_norm": 2.299131393432617,
      "learning_rate": 8.752404354577052e-06,
      "loss": 1.2009,
      "step": 475
    },
    {
      "epoch": 3.657142857142857,
      "grad_norm": 2.2763421535491943,
      "learning_rate": 8.301459239887074e-06,
      "loss": 1.181,
      "step": 480
    },
    {
      "epoch": 3.6952380952380954,
      "grad_norm": 2.3265256881713867,
      "learning_rate": 7.860117300586383e-06,
      "loss": 1.1409,
      "step": 485
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 2.3078391551971436,
      "learning_rate": 7.428632348317005e-06,
      "loss": 1.1747,
      "step": 490
    },
    {
      "epoch": 3.7714285714285714,
      "grad_norm": 2.101562261581421,
      "learning_rate": 7.007252526059446e-06,
      "loss": 1.1488,
      "step": 495
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 2.3537468910217285,
      "learning_rate": 6.596220165428002e-06,
      "loss": 1.1659,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 655,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9119063355254374e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
